{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 - Q2 (2)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrn9hcopjdWFeFgVWvOM45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchitkalra/bits-f312-nnfl/blob/main/Assignment_2_Q2_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_UUKn8xB5jl",
        "outputId": "329b5869-509a-4380-87be-f7bf334de2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "cl_X19IUB8LE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/drive/MyDrive/NNFL/Assignment2/data55.xlsx',header=None)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBisogHsB9cV",
        "outputId": "2948e236-823c-4b60-a919-db827e568ff8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0       1       2       3       4       5       6       7       8   \\\n",
            "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
            "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
            "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
            "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
            "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
            "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
            "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
            "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
            "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
            "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
            "\n",
            "         9   ...      51      52      53      54      55      56      57  \\\n",
            "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
            "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
            "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
            "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
            "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
            "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
            "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
            "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
            "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
            "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
            "\n",
            "         58      59  60  \n",
            "0    0.0090  0.0032   0  \n",
            "1    0.0052  0.0044   0  \n",
            "2    0.0095  0.0078   0  \n",
            "3    0.0040  0.0117   0  \n",
            "4    0.0107  0.0094   0  \n",
            "..      ...     ...  ..  \n",
            "203  0.0193  0.0157   1  \n",
            "204  0.0062  0.0067   1  \n",
            "205  0.0077  0.0031   1  \n",
            "206  0.0036  0.0048   1  \n",
            "207  0.0061  0.0115   1  \n",
            "\n",
            "[208 rows x 61 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datan = data.values\n",
        "\n",
        "np.random.shuffle(datan)\n",
        "\n",
        "X = datan[:, :60]\n",
        "print(X.shape)\n",
        "y = datan[:, 60]\n",
        "y = 2*y -1\n",
        "print(y.shape)\n",
        "\n",
        "# Split into train, test, and validation sets\n",
        "# 70% training, 20% testing, 10% validation\n",
        "X_train, _X, y_train, _y = train_test_split(X, y, test_size=0.30)\n",
        "X_test, X_val, y_test, y_val = train_test_split(_X, _y, test_size=0.33)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZV6g4VNB_es",
        "outputId": "d27b5a4b-cf5e-4570-f03c-652ee8f8251a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(208, 60)\n",
            "(208,)\n",
            "(145, 60)\n",
            "(42, 60)\n",
            "(21, 60)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_linear = np.zeros(X_train.shape[0])\n",
        "# print(weights_linear)\n",
        "b_linear = np.zeros(X_train.shape[0])\n",
        "# print(b_linear)\n",
        "\n",
        "weights_rbf = np.zeros(X_train.shape[0])\n",
        "# print(weights_rbf)\n",
        "b_rbf = np.zeros(X_train.shape[0])\n",
        "# print(b_rbf)\n",
        "\n",
        "weights_poly = np.zeros(X_train.shape[0])\n",
        "# print(weights_poly)\n",
        "b_poly = np.zeros(X_train.shape[0])\n",
        "# print(b_poly)"
      ],
      "metadata": {
        "id": "GBWMD_MRCBL8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poly_kernel(X1, X2):\n",
        "  return (np.dot(X1, X2)+1)**3"
      ],
      "metadata": {
        "id": "d74iuUf6DCCy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_kernel(X1, X2):\n",
        "  return np.dot(X1, X2)"
      ],
      "metadata": {
        "id": "1o5oQYZbnGwm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rbf_kernel(X1, X2):\n",
        "  sigma = 1\n",
        "  return np.exp((np.linalg.norm(X1-X2)**2)/(2*(sigma**2)))"
      ],
      "metadata": {
        "id": "g1bgCyfKnQvL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Polynominal kernel\n",
        "\n",
        "for k in range(10000):\n",
        "  for i in range(X_train.shape[0]):\n",
        "    hypothesis = np.tanh(np.sum(np.dot(weights_poly, poly_kernel(X_train[i], X_train.T))))\n",
        "    if np.sign(hypothesis) != y_train[i]:\n",
        "      weights_poly[i] = weights_poly[i] + y_train[i]\n",
        "      b_poly = b_poly + y_train"
      ],
      "metadata": {
        "id": "T_uqRkYZCz8N"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with RBF kernel\n",
        "\n",
        "for k in range(10000):\n",
        "  for i in range(X_train.shape[0]):\n",
        "    hypothesis = np.tanh(np.sum(np.dot(weights_rbf, rbf_kernel(X_train, X_train))))\n",
        "    if np.sign(hypothesis) != y_train[i]:\n",
        "      weights_rbf[i] = weights_rbf[i] + y_train[i]\n",
        "      b_rbf = b_rbf + y_train"
      ],
      "metadata": {
        "id": "lRq4teUJpQ1u"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Linear kernel\n",
        "\n",
        "for k in range(10000):\n",
        "  for i in range(X_train.shape[0]):\n",
        "    hypothesis = np.tanh(np.sum(np.dot(weights_linear, linear_kernel(X_train[i], X_train.T))))\n",
        "    if np.sign(hypothesis) != y_train[i]:\n",
        "      weights_linear[i] = weights_linear[i] + y_train[i]\n",
        "      b_linear = b_linear + y_train"
      ],
      "metadata": {
        "id": "QTmyIruWpXj0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weights_poly)\n",
        "print(b_poly)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2K8wEKuClji",
        "outputId": "3c6f1e3d-bb0f-4152-8561-33b49eb00e4c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1.   0.  -1.  -1.   0.  18. -10.   7.   0.   1.   2. -16.   5.  13.\n",
            "   0.  -2.   0.  -3.   2.   0.   0.   0.  -2.   1.   2.   0. -11.   2.\n",
            "   0.  -2.   0.   0. -21.  -1.   9.  -1.   5.   0.   5.  -9.  -9.  -2.\n",
            "   4.  -1.   7.   2.   0.   2.   0.  -4.   3.   0.   0.   0.   2. -13.\n",
            " -15.   0.   0.   0.   0.  14.   8.   0.   1.   0.  -3.   2.  -9.  -1.\n",
            "   0.   0.   0.   0.   0.  -2.  -3.   0.  -6.  20.  -1. -11.   2.   0.\n",
            "  -7.   7.   0.   0.   1.   0.  -7.  -2.   0. -15.   2.   0.  15. -18.\n",
            "   1.   9.   0.  -4.   0.   0.  -4.  20.   0.  21.  -1.   0.   0.   0.\n",
            "   0.  -1. -12.   0.   0.   0.  -3. -12.   6.   0.   0.  -8.   0.   1.\n",
            "   0.  17.  16.   0.   0.   0. -19.   1. -12.   5.   6.   0.   4.   0.\n",
            "   0.   1.   0.  -4.   0.]\n",
            "[ 562.  562. -562. -562. -562.  562. -562.  562.  562.  562.  562. -562.\n",
            "  562.  562.  562. -562.  562. -562.  562.  562. -562. -562. -562.  562.\n",
            "  562.  562. -562.  562.  562. -562.  562.  562. -562. -562.  562. -562.\n",
            "  562. -562.  562. -562. -562. -562.  562. -562.  562.  562.  562.  562.\n",
            "  562. -562.  562.  562.  562.  562.  562. -562. -562. -562. -562. -562.\n",
            " -562.  562.  562.  562.  562. -562. -562.  562. -562. -562. -562. -562.\n",
            " -562. -562. -562. -562. -562. -562. -562.  562. -562. -562.  562.  562.\n",
            " -562.  562.  562.  562.  562. -562. -562. -562. -562. -562.  562. -562.\n",
            "  562. -562.  562.  562. -562. -562. -562.  562. -562.  562. -562.  562.\n",
            " -562.  562.  562.  562.  562. -562. -562.  562.  562. -562. -562. -562.\n",
            "  562.  562.  562. -562.  562.  562. -562.  562.  562.  562.  562.  562.\n",
            " -562.  562. -562.  562.  562.  562.  562.  562.  562.  562.  562. -562.\n",
            "  562.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[3]"
      ],
      "metadata": {
        "id": "40kdXV4bFNNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing for Polynominal kernel\n",
        "\n",
        "arr = []\n",
        "\n",
        "for k in range(X_test.shape[0]):\n",
        "  hypothesis = np.tanh(np.sum(np.dot(weights_poly, poly_kernel(X_test[k], X_train.T))))\n",
        "  arr.append(hypothesis)\n",
        "\n",
        "yp = np.array(arr)\n",
        "\n",
        "cm=confusion_matrix(y_test, yp)\n",
        "print(cm)\n",
        "accuracy=(cm[0][0]+cm[1][1])/(len(y_test))\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6iOe_ddGBhQ",
        "outputId": "78d77d9c-7ae9-4310-ecce-0ec0409bac6b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14  2]\n",
            " [ 2 24]]\n",
            "0.9047619047619048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing for RBF kernel\n",
        "\n",
        "arr = []\n",
        "\n",
        "for k in range(X_test.shape[0]):\n",
        "  hypothesis = np.tanh(np.sum(np.dot(weights_rbf, rbf_kernel(X_test, X_train))))\n",
        "  arr.append(hypothesis)\n",
        "\n",
        "yp = np.array(arr)\n",
        "\n",
        "cm=confusion_matrix(y_test, yp)\n",
        "print(cm)\n",
        "accuracy=(cm[0][0]+cm[1][1])/(len(y_test))\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "x__AdXTSpqdh",
        "outputId": "0273e329-0580-4238-961c-cd46588858f8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-67b287060695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_rbf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-5bda2715ccbf>\u001b[0m in \u001b[0;36mrbf_kernel\u001b[0;34m(X1, X2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (42,60) (145,60) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing for Linear kernel\n",
        "\n",
        "arr = []\n",
        "\n",
        "for k in range(X_test.shape[0]):\n",
        "  hypothesis = np.tanh(np.sum(np.dot(weights_linear, linear_kernel(X_test[k], X_train.T))))\n",
        "  arr.append(hypothesis)\n",
        "\n",
        "yp = np.array(arr)\n",
        "print(yp)\n",
        "\n",
        "cm=confusion_matrix(y_test, yp)\n",
        "print(cm)\n",
        "accuracy=(cm[0][0]+cm[1][1])/(len(y_test))\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "yoHzEdRUpyr7",
        "outputId": "e1414421-f189-4133-dac3-094d103df954"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.         -0.99999561  1.         -0.99999809  1.          1.\n",
            "  0.99949002  0.99999998  1.          1.         -1.          1.\n",
            "  1.         -1.          1.         -1.          0.65724476 -1.\n",
            "  1.          1.         -0.97704093 -1.         -0.9999988  -0.35095396\n",
            " -1.          1.          1.          0.99996712  1.          1.\n",
            " -0.99999999  1.          1.          1.         -0.99999998  1.\n",
            " -1.          1.          1.         -1.         -0.99999897 -0.99999397]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-2b167c23696c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
          ]
        }
      ]
    }
  ]
}